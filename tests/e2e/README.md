# ContentCache End-to-End Tests

## Purpose

Black-box end-to-end validation that the Rust VNC viewer (`njcvncviewer-rs`) exhibits the same ContentCache behavior as the C++ viewer (`njcvncviewer`). The test compares cache hit rates, protocol message patterns, and ARC statistics between the two implementations.

## Architecture

**VNC-in-VNC Setup**: Two TigerVNC servers run simultaneously:
- **Display :998** (port 6898): Content generation server with automated desktop interactions
- **Display :999** (port 6899): Nested VNC server running an internal viewer connected to :998

External test viewers connect to :999 and experience the ContentCache protocol messages generated by :999 based on content from :998.

## Requirements

### System Packages

**Required**:
```bash
sudo apt-get install tigervnc-standalone-server tigervnc-server \
  xterm openbox xset root wmctrl xdotool
```

**Optional** (for screenshot comparison):
```bash
sudo apt-get install xvfb x11-apps imagemagick
```

**Fonts** (for consistent rendering):
```bash
sudo apt-get install xfonts-base xfonts-75dpi xfonts-100dpi
```

### TigerVNC Viewers

Both viewers must be built before running tests:

```bash
# C++ viewer
make viewer

# Rust viewer
make rust_viewer
```

### Safety Notes

⚠ **CRITICAL**: This test framework **NEVER touches** production VNC servers on displays :1, :2, or :3. It only manages processes it starts on high-numbered displays (:998, :999).

All processes are launched in their own process groups and tracked explicitly. The framework never uses broad `pkill` commands.

## Usage

### Standalone Execution

```bash
# Basic run (default 90 second scenario)
python3 tests/e2e/run_contentcache_test.py

# Verbose output
python3 tests/e2e/run_contentcache_test.py --verbose

# Custom duration
python3 tests/e2e/run_contentcache_test.py --duration 120

# Custom displays/ports
python3 tests/e2e/run_contentcache_test.py --display1 900 --port1 6800 \
  --display2 901 --port2 6801
```

## Server Modes

- `system`: Use system-installed Xtigervnc
- `local`: Use project build `build/unix/vncserver/Xnjcvnc` if present
- `auto` (default): Run `system` and also `local` if available

CLI:
```bash
# Run system and local and compare
./run_contentcache_test.py --server-modes system,local
```

### CTest Integration

```bash
# Run via CTest
ctest --test-dir build -R e2e_contentcache -V

# Run all e2e tests
ctest --test-dir build -L e2e --output-on-failure
```

## Test Flow

1. **Preflight**: Check dependencies (Xtigervnc, xterm, wmctrl, viewers, etc.)
2. **Start :998**: Launch content generation server + window manager + session
3. **Start :999**: Launch nested VNC server + window manager
4. **Internal viewer**: Start C++ viewer in :999 connecting to :998 (logs suppressed)
5. **C++ baseline**: Run external C++ viewer connecting to :998, capture logs
6. **Scenario**: Execute automated desktop interactions on :998 (xterm-based deterministic content)
7. **Rust candidate**: Replay scenario with external Rust viewer, capture logs
8. **Repeat per server mode**: Run the above for each server mode (system Xtigervnc, local Xnjcvnc)
9. **Comparison**: Parse logs and compare metrics across viewers and server modes
10. **Cleanup**: Gracefully terminate all processes

## Expected Log Patterns

### ContentCache Protocol Messages

**C++ viewer**:
```
DecodeManager: Cache hit for ID 12345: blitting 64x64 to [100,100-164,164]
DecodeManager: Cache miss for ID 67890, requesting from server
DecodeManager: Storing decoded rect [200,200-264,264] with cache ID 11111
```

**Rust viewer** (similar):
```
[INFO] ContentCache hit: cache_id=12345 rect=[100,100,64,64]
[INFO] ContentCache miss: cache_id=67890, sending request
[INFO] Stored decoded rect with cache_id=11111
```

### ARC Statistics

End-of-session summary:
```
Client-side ContentCache statistics:
  Protocol operations (CachedRect received):
    Lookups: 1523, Hits: 1234 (81.0%)
    Misses: 289
  ARC cache performance:
    T1 (recency): 450 entries, T2 (frequency): 784 entries
```

## Validation Criteria

The test **passes** if:
- ✓ No crashes or fatal errors
- ✓ Cache hit rate within ±2% (absolute percentage points)
- ✓ Protocol message counts within ±5% (`CachedRect`, `CachedRectInit`, `RequestCachedData`)
- ✓ ARC T1/T2 balance within ±10%
- ✓ Framebuffer screenshots pixel-identical (if Xvfb available)

The test **fails** if:
- ✗ Crash or hang during execution
- ✗ Hit rate divergence > 2%
- ✗ Protocol message count divergence > 5%
- ✗ Parsing errors or corrupted logs

## Troubleshooting

### Port Conflicts

**Error**: `ERROR: Port 6898 already in use`

**Solution**: Check for existing processes:
```bash
lsof -i :6898
# If found, stop the conflicting process or use different ports:
python3 tests/e2e/run_contentcache_test.py --port1 7000 --port2 7001
```

### Display Conflicts

**Error**: `ERROR: Display :998 already in use`

**Solution**:
```bash
ls /tmp/.X11-unix/X998
# If exists:
rm /tmp/.X11-unix/X998  # Only if you're sure it's stale
# Or use different display:
python3 tests/e2e/run_contentcache_test.py --display1 950 --display2 951
```

### Window Manager Fails to Start

**Error**: `ERROR: Window manager failed to start`

**Solution**: Install openbox or try fluxbox:
```bash
sudo apt-get install openbox
# Or:
sudo apt-get install fluxbox
python3 tests/e2e/run_contentcache_test.py --wm fluxbox
```

### Low Hit Rate

**Symptom**: Test passes but hit rate is < 20%

**Causes**:
- Scenario duration too short (not enough repeated content)
- ContentCache disabled on server
- MinRectSize threshold too high

**Solution**:
```bash
# Increase duration for more cycles
python3 tests/e2e/run_contentcache_test.py --duration 180

# Check server parameters (EnableContentCache=1, ContentCacheMinRectSize=4096)
```

### Screenshot Differences

**Error**: `Framebuffer comparison FAIL: 1234 pixels differ`

**Causes**:
- Font rendering variations (different font packages installed)
- Anti-aliasing differences
- Timing issues (windows not fully rendered)

**Solution**:
```bash
# Install consistent fonts
sudo apt-get install xfonts-base xfonts-75dpi xfonts-100dpi

# Increase quiet period in scenarios.py (wait_idle after scenario)
```

If pixel-perfect comparison is not critical, the test can still pass on log comparison alone.

### Missing Dependencies

**Error**: `PreflightError: Required binary not found: xdotool`

**Solution**: Install missing packages as listed in Requirements section above.

## Specific Bug Tests

### CachedRectInit Propagation Test

**Issue**: Server has ContentCache hits but sends 0 `CachedRect` references to clients.

**Test**: `test_cachedrectin it_propagation.py`

Validates that when the server has a cache hit, it properly communicates it to the client via either:
- `CachedRect` (if client knows the cacheId)
- `CachedRectInit` (if client doesn't know the cacheId yet)

**Usage**:
```bash
# Server-side validation only
python3 tests/e2e/test_cachedrectin\ it_propagation.py --display 998 --port 6898

# Full protocol flow validation with viewer
python3 tests/e2e/test_cachedrectin\ it_propagation.py --display 998 --port 6898 --with-viewer

# Custom duration
python3 tests/e2e/test_cachedrectin\ it_propagation.py --duration 120 --with-viewer
```

**Test will FAIL if**:
- Server performs cache lookups but sends 0 references
- Client receives no `CachedRect` or `CachedRectInit` messages despite server cache hits

**Test will PASS if**:
- Server cache lookups result in cache reference messages being sent
- Client receives matching number of cache protocol messages

**Bug identified**: 2025-11-03 - Server queues `CachedRectInit` messages when client doesn't know a cacheId, but those queued messages are not being transmitted properly.

## Extending Scenarios

To add new test scenarios:

1. **Edit `scenarios.py`**: Add a new method to `ScenarioRunner`:
   ```python
   def my_custom_scenario(self, duration_sec: float = 60.0) -> dict:
       """Custom scenario description."""
       stats = {}
       # Your scenario implementation
       return stats
   ```

2. **Call from orchestrator**: Modify `run_contentcache_test.py` to invoke your scenario:
   ```python
   runner = ScenarioRunner(display=998, verbose=args.verbose)
   stats = runner.my_custom_scenario(duration_sec=args.duration)
   ```

3. **Test independently**: Run your scenario in isolation:
   ```python
   DISPLAY=:998 python3 -c "
   from scenarios import ScenarioRunner
   r = ScenarioRunner(998, verbose=True)
   r.my_custom_scenario(duration_sec=30)
   "
   ```

## Artifacts

All logs, screenshots, and reports are saved to:
```
tests/e2e/_artifacts/<timestamp>/
  logs/
    content_xtigervnc_998.log
    nested_xtigervnc_999.log
    content_wm.log
    nested_wm.log
    cpp_viewer.log
    rust_viewer.log
  screenshots/
    cpp_viewer.png
    rust_viewer.png
    fb_diff.png  # If framebuffers differ
  reports/
    e2e_contentcache_report.html
```

The artifact path is printed at the start of the test run.

## Implementation Status

### Completed Modules
- ✅ `framework.py`: Server lifecycle, process tracking, preflight checks
- ✅ `scenarios.py`: Automated content generation (xterm open/close/type cycles)
- ✅ `log_parser.py`: Log parsing and normalization for ContentCache events
- ✅ `comparator.py`: Tolerance-based metric comparison

### TODO: Remaining Work
- ⏳ `run_contentcache_test.py`: Main orchestrator (VNC-in-VNC setup, viewer execution, comparison)
- ⏳ `report.py`: HTML report generation with embedded screenshots
- ⏳ `CMakeLists.txt`: CTest integration
- ⏳ Screenshot comparison logic (ImageMagick integration)
- ⏳ Coredump capture on crash

To complete the implementation:
1. Implement the main orchestrator in `run_contentcache_test.py` following the flow described above
2. Add HTML report generation in `report.py`
3. Create `CMakeLists.txt` for CTest integration
4. Test the full pipeline end-to-end
5. Iterate on log parsing patterns as actual log format is refined

## Contact

For questions or issues with the e2e test framework, refer to the project's WARP.md for ContentCache documentation and known issues.
